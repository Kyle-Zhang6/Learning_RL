## Actor-Critic算法实现笔记

- #### 2020.12.05

  实现Actor-Critic属实不容易，最开始尝试实现__动作价值AC__时，模型没法很好收敛，甚至直接发散...（CartPole一直只能完成8-11步）。于是这几天一直在给模型添油加醋，什么目标网络、经验回放云云全部加上，希望能加强模型收敛性，然而收效甚微。从怀疑自己不会用pytorch开始，到怀疑算法本身性能太差，整个人都很难受。今天决定不再盲目地往原来的模型上加东西了，把所提到的算法都一个个地写出来，不收敛也无可奈何，然而发现了新大陆。

  ##### 今日完成：

  __1. "actor_critic_action_value.py" 基于动作价值的AC__

     - 使用下一状态(state_next)与下一状态下的动作(next_action)估计回报值。

   - 最开始这个模型直接发散，今天急中生智开始调参，发现Critic的学习率提高一点，Actor学习率降低一点可以非常有效地帮助模型收敛（至少可以多次达到200地reward了！）。我想，Actor用来根据某个状态下所有动作的概率分布选择一个动作，Critic用来评价某一状态某个动作的价值。很显然，只有在Critic较为精确的情况下，才能指导Actor做出更正确的决定。因此需要让Critic更快地进行收敛，Actor可以慢慢收敛，一是等待Critic能更准确地指导自己，防止自己在不正确的指导下__过分强化了不正确的策略__，二是可以在前期__保持探索性__（慢慢收敛可以在前期让某状态下的动作概率分布较平均）。

   - 并没有继续调参，但是事实证明，该代码能够对网络起到训练效果！

     

- #### 2020.12.08

  这些日子一直有事，先是做表达与交流PPT，而后开始申请ETH / EPFL / 哥大 （双手合十请求offer），事实上A2C的算法在前几天就写完了，主要内容是把回报值__U__使用__td_error__代替了。但是这直接使得模型难以收敛，贼烦。Actor策略和Critic状态价值的估计一直在震荡，怎么调参都没法实现收敛。不知道是什么地方出现了问题。

  所以就接着往下写吧，把OPP / SAC什么的算法学了并复现出来，希望能够成功实现CartPole。

  